{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Prediction Model using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 of Q2 focuses on the topic of neural networks. Specifically, you will be building a neural network regression model to predict the stock returns using reported ratios from earnings announcement. Through this short exercise, you will get familiar with the basics of neural networks using [scikit-learn](https://scikit-learn.org/stable/), which is an open-source machine learning library for Python. Data are obtained from [WRDS](https://wrds-www.wharton.upenn.edu/) and are pre-processed for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are flexible ML algorithms commonly used in both classification and regression tasks, which are characterized by significant non-linearities and complex interactions amongst variables in the feature set. For the neural network model in `sklearn`, the architecture underlying the neural network model is the **multi-layer perceptron (MLP)** algorithm.\n",
    "\n",
    "The MLP algorithm learns a function $f(\\cdot):R^m \\rightarrow R^o$ by training on a dataset with an $m$-dimensional input $X = x_1, x_2, ..., x_n$, and an $o$-dimensional output $y$. The basic structure of the MLP algorithm (taken from the [scikit-learn](https://scikit-learn.org/stable/) website) is shown below:\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leftmost layer is the *input* layer, consisting of a set of neurons $\\{x_1, x_2, ..., x_n\\}$ that represent the input features. \n",
    "\n",
    "In between the input layer and the output layer are *hidden layers*, where learning occurs by transforming the values from the previous layer into a weighted linear summation $w_1 x_1 + w_2 x_2 + ... w_n x_n$. \n",
    "\n",
    "An *activation function* $g(\\cdot):R \\rightarrow R$ then transforms the sum and feeds it to the output layer. Note that the above diagram has only one hidden layer, while most neural networks for practical use have 2 or more hidden layers, each feeding the output from the previous layer to the next one, in a transmission process called *forward propagation*.\n",
    "\n",
    "When training a neural network, a neural network's predictions are compared to the actual values of labeled data, and then evaluated based on some performance measure that calculates the error of the neural network. Then, the weights for each layer of the neural network is adjusted to reduce the error of the network, and the process is repeated for a specified number of iterations, in which a neural network gradually learns and adjusts its weights to create predictions that better fit the training set. The *learning rate* affects the magnitude of adjustments for each iteration of the training process. \n",
    "\n",
    "In regards to the hyperparameters of the neural network, we can specify the exact activation function used to transform the output at each layer, such as `relu` or `tanh`, as well as the *solver* used for weight optimization, such as `lbfgs`, `sgd`, and `adam`. Furthermore, we can specify the number of iterations and learning rate for the training process. For the latter, we can specify both the initial learning rate, as well as whether the learning rate is constant or adaptive to decreases in training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the libraries. We will use the scikit-learn package to run LASSO in Python. If you are not using the Anaconda distribution, you might need to navigate to [this page](https://scikit-learn.org/stable/install.html) and follow the instructions to install the correct version for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the first dataset on daily stock returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security = pd.read_csv(\"PSet2Q2data_stock.csv\")\n",
    "\n",
    "security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the second dataset on firm financial ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = pd.read_csv(\"PSet2Q2data_firm.csv\")\n",
    "\n",
    "ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the datasets by date and get rid of missing values and redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ratios.merge(security, on='public_date').dropna(axis=1)\n",
    "merged['ret2'] = merged['ret']\n",
    "merged = merged.drop(['ret', 'id', 'date'], axis=1)\n",
    "\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the dataset into a training set (roughtly 90% of data) and a test set (roughly 10% of data). Since stock data is time-series, we can only use past information as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = merged.iloc[:, 3:-1]\n",
    "y = merged.iloc[:,-1]\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to note that the Multi-layer Perceptron model for our neural network is highly sensitive to feature scaling. Thus, we first need to pre-process and scale our data in order to ensure we get satisfactory results. We first call `StandardScaler` in order to build the scaling method to ensure each variable in our feature set of independent variables has mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the scaler to scale the x variables in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Neural Network\n",
    "Now we can begin the training process. First, let *clf* be a MLP regression model with 4 hidden layers of size 250, 150, 150, and 100. It also uses the `tanh` activation function for the hidden layer, and the `adam` solver for weight optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(hidden_layer_sizes=(250,150,150,100),\n",
    "                       max_iter = 300, activation = 'tanh',\n",
    "                       solver = 'adam', random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the fitted model to create your predictions and calculate the score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score for our current model is very low, so let us see if we can find a better model. Using grid search, we can perform hyperparameter optimization to find if there is a better set of hidden layers, activation functions, solvers, learning rates, and other hyperparameters that can give better results to our model. \n",
    "\n",
    "First, we specify the grid of hyperparameters to train our neural network model over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(250,150,150,100), (100,100,100,100), (150,100,100,50), (250,150,50), (150,100,50)],\n",
    "    'max_iter': [300, 500],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.005],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid, n_jobs= -1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model on our training set over the entire grid of hyperparameters, which will give us the set of hyperparameters that gives the best results during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(x_tr, y_tr)\n",
    "\n",
    "print(grid.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fitted model from the grid search to predict y. Store the predictions in *grid_pred*, and output the score for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the model that gave the best results from hyperparameter optimization, with as much detail about the specifics of the hyperparameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "```\n",
    "\n",
    "```\n",
    "#</GRADED>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
